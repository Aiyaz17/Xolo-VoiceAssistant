{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_kM5G-m6_ghk"
      },
      "outputs": [],
      "source": [
        " import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZvCqDZ6_zw0"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"CoQA_data.csv\"):\n",
        "  new_df = pd.read_csv(\"CoQA_data.csv\")\n",
        "\n",
        "else:\n",
        "    coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
        "    coqa.head()\n",
        "    del coqa[\"version\"]\n",
        "    cols = [\"text\",\"question\",\"answer\"]#list of lists to create our dataframe\n",
        "    comp_list = []\n",
        "    for index, row in coqa.iterrows():\n",
        "        for i in range(len(row[\"data\"][\"questions\"])):\n",
        "            temp_list = []\n",
        "            temp_list.append(row[\"data\"][\"story\"])\n",
        "            temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
        "            temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
        "            comp_list.append(temp_list)\n",
        "            new_df = pd.DataFrame(comp_list, columns=cols) #saving the dataframe to csv file for further loading\n",
        "    new_df.to_csv(\"CoQA_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VC20WlxUARjE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of question and answers:  108647\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of question and answers: \", len(new_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iKVDEUdLAbBU"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\Administrator/.cache\\huggingface\\transformers\\28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e' at 'C:\\Users\\Administrator/.cache\\huggingface\\transformers\\28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:349\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=347'>348</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=348'>349</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=349'>350</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=711'>712</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=712'>713</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:938\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=936'>937</a>\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=937'>938</a>\u001b[0m typed_storage\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49m_set_from_file(\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=938'>939</a>\u001b[0m     f, offset, f_should_read_directly,\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=939'>940</a>\u001b[0m     torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(typed_storage\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/torch/serialization.py?line=940'>941</a>\u001b[0m \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: unexpected EOF, expected 1684472 more bytes. The file might be corrupted.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:353\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=351'>352</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=352'>353</a>\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=353'>354</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=354'>355</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=355'>356</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=356'>357</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=357'>358</a>\u001b[0m         )\n",
            "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Program%20Files/Python310/lib/encodings/cp1252.py?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> <a href='file:///c%3A/Program%20Files/Python310/lib/encodings/cp1252.py?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2063: character maps to <undefined>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32md:\\PersonalPlayground\\rizviCollege\\second_year\\Xolo\\miniproject-final.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PersonalPlayground/rizviCollege/second_year/Xolo/miniproject-final.ipynb#ch0000004?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m BertForQuestionAnswering\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PersonalPlayground/rizviCollege/second_year/Xolo/miniproject-final.ipynb#ch0000004?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:1797\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1793'>1794</a>\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1794'>1795</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded:\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1795'>1796</a>\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1796'>1797</a>\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1797'>1798</a>\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1798'>1799</a>\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1799'>1800</a>\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1800'>1801</a>\u001b[0m     \u001b[39m#    weights entry - we assume all weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1801'>1802</a>\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=1802'>1803</a>\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:365\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=359'>360</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=360'>361</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate the file \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m which is necessary to load this pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=361'>362</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=362'>363</a>\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=363'>364</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeDecodeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=364'>365</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=365'>366</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=366'>367</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=367'>368</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Administrator/AppData/Roaming/Python/Python310/site-packages/transformers/modeling_utils.py?line=368'>369</a>\u001b[0m     )\n",
            "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\Administrator/.cache\\huggingface\\transformers\\28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e' at 'C:\\Users\\Administrator/.cache\\huggingface\\transformers\\28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
          ]
        }
      ],
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YHw2KK9Am9Y"
      },
      "outputs": [],
      "source": [
        "random_num = np.random.randint(0,len(new_df))\n",
        "question = new_df[\"question\"][random_num]\n",
        "text = new_df[\"text\"][random_num]\n",
        "\n",
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiJ1B8AwA71U"
      },
      "outputs": [],
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
        "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))\n",
        "\n",
        "#tokens with highest start and end scores\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "if answer_end >= answer_start:\n",
        "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "    \n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H42gaYcMBN_0"
      },
      "outputs": [],
      "source": [
        "answer = tokens[answer_start]\n",
        "for i in range(answer_start+1, answer_end+1):\n",
        "    if tokens[i][0:2] == \"##\":\n",
        "        answer += tokens[i][2:]\n",
        "    else:\n",
        "        answer += \" \" + tokens[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0If8BOuBVnU"
      },
      "outputs": [],
      "source": [
        "def question_answer(question, text):\n",
        "    \n",
        "    #tokenize question and text as a pair\n",
        "    input_ids = tokenizer.encode(question, text)\n",
        "    \n",
        "    #string version of tokenized ids\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    \n",
        "    #segment IDs\n",
        "    #first occurence of [SEP] token\n",
        "    sep_idx = input_ids.index(tokenizer.sep_token_id)    #number of tokens in segment A (question)\n",
        "    num_seg_a = sep_idx+1    #number of tokens in segment B (text)\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    \n",
        "    #list of 0s and 1s for segment embeddings\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b    \n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    \n",
        "    #model output using input_ids and segment_ids\n",
        "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
        "    \n",
        "    #reconstructing the answer\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits)    \n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "                \n",
        "    if answer.startswith(\"[CLS]\"):\n",
        "        answer = \"Unable to find the answer to your question.\"\n",
        "    \n",
        "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5coVghXxFMao"
      },
      "outputs": [],
      "source": [
        "text = \"name is Ajinkya and height is 180cm \"\n",
        "question = input(\"\\nPlease enter your question: \\n\")\n",
        "while True:\n",
        "    question_answer(question, text)\n",
        "    \n",
        "    flag = True\n",
        "    flag_N = False\n",
        "    \n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"\\nPlease enter your question: \\n\")\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nBye!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "            \n",
        "    if flag_N == True:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mini project sem IV.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
